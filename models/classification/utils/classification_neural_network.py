# -*- coding: utf-8 -*-


# -- Данный код предназначен для загрузки модели в Yandex DataSphere, и не ориентирован для дальнейших экспериментов --


"""classification_neural_network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mKX3VZRPfFSa-JW0ApkZY-K0YQjsP7Ds

## Выбор модели для задач классификации

Для задач классификации существует несколько способов выбора модели:

1. **Обучить маленькую модель с нуля**: Выбор легковесной архитектуры модели и полное обучение её на нашем наборе данных.
2. **Fine-Tuning предобученной модели**: Используем модель, которая была предобучена авторами на другом наборе данных, и только последние слои переобучается на нашем конкретном наборе данных.
3. **Использовать предобученные веса напрямую**: В этом методе используется предобученная модель без дополнительного обучения.

Лучшим вариантом является Fine-Tuning, поэтому в качестве бейзлайна возьмем маленькую (5.3M) EfficientNet B0 с претрейновыми параметрами

[Оригинальная статья](https://arxiv.org/abs/1905.11946)
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, transforms
from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights
from PIL import Image

import sys
import os
import zipfile
import json

if __name__ == '__main__':
    # Загрузка гиперпараметров из JSON (создайте папку classification, если её еще нет в корне диска Google Drive)
    with open('hyperparams.json', "r") as f:
      hyperparams = json.load(f)

    """
    **Структура** classification_dataset.zip*: (внутри папка с таким же названием)
    
    ```
    classification_dataset/
    ├── train/
    │   ├── 0/
    │   └── 1/
    ```
    """
    local_zip = 'classification_dataset.zip'
    zip_ref = zipfile.ZipFile(local_zip, 'r')
    zip_ref.extractall()
    zip_ref.close()

    # Класс для аугментации данных
    class AugmentedDataset(Dataset):
        def __init__(self, original_folder, target_size=50, transform=None):
            self.transform = transform
            self.samples = []
            self.class_to_idx = {}

            class_names = sorted(os.listdir(original_folder)) # Получаем отсортированный список имен классов
            for idx, class_name in enumerate(class_names): # Итерируемся и присваиваем индексы
                self.class_to_idx[class_name] = idx

            # Собираем пути к изображениям для каждого класса
            for class_name in class_names: # Используем отсортированные имена классов
                class_path = os.path.join(original_folder, class_name)
                images = [os.path.join(class_path, img) for img in os.listdir(class_path)]
                # Повторяем изображения до достижения целевого размера
                for i in range(target_size):
                    self.samples.append((images[i % len(images)], class_name))

        # Возвращает общее количество элементов в наборе данных
        def __len__(self):
            return len(self.samples)

        # Возвращает одно изображение и его метку по индексу
        def __getitem__(self, idx):
            img_path, class_name = self.samples[idx]
            image = Image.open(img_path).convert('RGB')

            # Применяем аугментации
            if self.transform:
                image = self.transform(image)

            # Преобразуем метку класса в числовой формат
            label_str = class_name # Метка класса все еще строка
            label = self.class_to_idx[label_str] # Используем class_to_idx для получения числового индекса
            return image, torch.tensor(label, dtype=torch.float32)

    # Трансформы с аугментациями для тренировочных данных
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Создание аугментированных датасетов
    train_dataset = AugmentedDataset(
        original_folder='classification_dataset/train',
        target_size=50,
        transform=train_transform
    )

    # DataLoader'ы
    train_loader = DataLoader(
        train_dataset,
        batch_size=hyperparams['batch_size'],
        shuffle=True
    )

    # Проверим классы
    print(train_dataset.class_to_idx)

    # Загрузка модели EfficientNet
    model = efficientnet_b0(EfficientNet_B0_Weights.DEFAULT)

    # Заморозка всех слоев, кроме последнего
    for param in model.parameters():
        param.requires_grad = False

    # Замена финального классификатора
    num_features = model.classifier[1].in_features
    model.classifier = nn.Sequential(
        nn.Linear(num_features, 128),
        nn.ReLU(),
        nn.Linear(128, 1),  # Бинарная классификация
        nn.Sigmoid()        # Для вероятностей
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    criterion = nn.BCELoss()  # Для бинарной классификации
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=hyperparams['learning_rate'],
        weight_decay=hyperparams['weight_decay']
    )

    # Обучение модели
    for epoch in range(hyperparams['num_epochs']):
        model.train()
        train_loss = 0.0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device).float()

            optimizer.zero_grad()
            outputs = model(images).squeeze()
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * images.size(0)

        # Валидация (НА TRAIN ДАТАСЕТЕ!)
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in train_loader:
                images, labels = images.to(device), labels.to(device).float()

                outputs = model(images).squeeze()
                loss = criterion(outputs, labels)

                predicted = (outputs > 0.5).float()
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

        # Вывод статистики
        train_loss = train_loss / len(train_loader.dataset)
        accuracy = correct / total

        print(f"Epoch {epoch+1}/{hyperparams['num_epochs']}")
        print(f"Train Loss: {train_loss:.4f}, Accuracy: {accuracy:.4f}")

    # Сохранение модели
    torch.save(model, 'trained_model_classification.pt')

    print("Модель сохранена")
    sys.exit(0)