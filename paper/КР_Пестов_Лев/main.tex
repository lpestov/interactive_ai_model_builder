\documentclass[14pt]{extarticle}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[left=2cm,right=2cm, top=2cm,bottom=2cm]{geometry}
\usepackage[english,russian]{babel}
\usepackage[pdftex,unicode=true,colorlinks,filecolor=black,citecolor=black,linkcolor=black]{hyperref}
\usepackage{enumitem}
\usepackage[backend=biber,style=numeric,sorting=none]{biblatex}
\addbibresource{references.bib}
\usepackage{titlesec}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}                   
  {1em}                             
  {}
\titleformat{\subsection}
  {\normalfont\normalsize\itshape\bfseries}  
  {\thesubsection.}                 
  {1em}                            
  {}                               


\setcounter{secnumdepth}{5}
\setlength{\parskip}{10pt}
\begin{document}
\begin{titlepage}
\begin{center}
\normalsize
\normalsize{ФЕДЕРАЛЬНОЕ ГОСУДАРСТВЕННОЕ АВТОНОМНОЕ\\ОБРАЗОВАТЕЛЬНОЕ УЧРЕЖДЕНИЕ\\ВЫСШЕГО ОБРАЗОВАНИЯ\\«НАЦИОНАЛЬНЫЙ ИССЛЕДОВАТЕЛЬСКИЙ УНИВЕРСИТЕТ\\

ВЫСШАЯ ШКОЛА ЭКОНОМИКИ»}

\vfill

\textbf{Факультет информатики, математики и компьютерных наук}\\[3mm]

\textbf{Программа подготовки бакалавров по направлению\\Компьютерные науки и технологии}

\vfill

\textit{Пестов Лев Евгеньевич}\\[3mm]

\textbf{КУРСОВАЯ РАБОТА}\\[10mm]

\normalsize{ 
 	
Интерактивный конструктор моделей искусственного интеллекта с поддержкой мультимодальных задач. \\Реализация пайплайна обучения глубоких нейросетей}

\end{center}

\vfill
\newlength{\ML}
\settowidth{\ML}{«\underline{\hspace{0.7cm}}»
\underline{\hspace{2cm}}}
\hfill
\begin{minipage}{0.4\textwidth}
\raggedleft{Научный руководитель\\
старший преподаватель НИУ ВШЭ - НН\\[2mm]
Саратовцев Артём Романович}
\end{minipage}%
\vfill
\begin{center}

Нижний Новгород, 2025г.

\end{center}
\end{titlepage}
\newpage  

\renewcommand{\contentsname}{Структура работы}
\tableofcontents
\newpage

\section{Введение}
\hspace{1cm}
Развитие искусственного интеллекта (ИИ) и его интеграция в различные сферы деятельности привело к значительному увеличению числа приложений, использующих глубокие нейронные сети. Однако процесс разработки таких моделей по-прежнему остается сложной и ресурсоемкой задачей, требующей от разработчика не только понимания архитектуры нейросетей, но и глубоких знаний в области обработки данных, настройки гиперпараметров и оптимизации вычислительных ресурсов. Это существенно ограничивает доступность технологий машинного обучения для широкой аудитории, в частности для исследователей, предпринимателей и специалистов, не обладающих достаточным уровнем подготовки в данной области.

Данная работа направлена на создание интерактивного конструктора моделей искусственного интеллекта, который позволит пользователям, не имеющим значительного опыта в разработке ИИ, самостоятельно обучать нейросети на своих данных под определенные классы задач. В отличие от существующих решений, сервис ориентирован на адаптацию моделей к малым объемам данных, что особенно актуально в условиях ограниченности ресурсов и сложности сбора крупных размеченных датасетов.

Существенной частью системы является интеграция с облачными вычислительными мощностями Yandex Cloud, что позволяет пользователям запускать обучение моделей удалённо, без необходимости наличия высокопроизводительного оборудования. Впоследствии обученные модели могут быть загружены обратно в сервис для тестирования либо экспортированы в виде предобученных весов с преднаписанным кодом для внедрения в сторонние проекты.

Целью исследования является сравнение различных архитектур моделей и их обучение, с учетом того, что ресурсы ограничены, а также создание скриптов с динамическим изменением гиперпараметров обучения и генеральной совокупности данных для подачи на бекенд-часть сервиса.

Задачи проекта:
\begin{enumerate}
    \item  Провести аналитический обзор литературы и выбрать лучшие архитектуры моделей под разные домены машинного обучения, также рассмотреть способы аугментации данных.
    \item  Выбрать способ обучения моделей, а также реализовать полное/fine-tuning обучение разных архитектур.
    \item Сравнить полученные результаты полученные на разных данных/методах обучения и сделать оптимальный выбор модели под задачи сервиса.
\end{enumerate}


\newpage
\section{Разработка пайплайна обучения нейросетей для задачи классификации изображений}

\subsection{Теоретические основы и архитектуры нейросетей для классификации изображений}
\hspace{1cm}
Задача классификации изображений является одной из основных в компьютерном зрении и представляет собой процесс экстракции высокоуровневых и низкоуровневых признаков в один или несколько классов. Для решения таких задач в последние годы наиболее эффективными оказались глубокие нейронные сети, в частности, сверточные нейронные сети (CNN) и трансформеры (ViT).

\textbf{Сверточные нейронные сети} (Convolutional Neural Networks, CNN) - это класс глубоких нейронных сетей, специально разработанных для обработки изображений. Их ключевой особенностью является использование операции свертки вместо обычного матричного умножения хотя бы в одном из слоев. CNN произвели революцию в области компьютерного зрения, предоставив эффективный способ автоматического извлечения признаков из изображений.

\textbf{Операция свертки} - Фильтр перемножает числа своей матрицы и матрицы картинки, далее они суммируются в одно число, процесс итеративно продолжается и тем самым получается новая матрица изображения.

\hspace{1cm}
Архитектура \textbf{AlexNet} — первая глубокая сверточная нейронная сеть, которая значительно превзошла предыдущие подходы к распознаванию изображений. Предложенная в 2012 году \cite{krizhevsky2012imagenet}, она стала прорывом в задачах классификации изображений, впервые показав возможности глубоких нейронных сетей превзойти классические методы машинного обучения на соревновании ImageNet. AlexNet состоит из 5 сверточных слоёв и 3 полносвязаных слоёв.

\hspace{1cm}
\textbf{VGGNet} — продолжение развития сверточных нейросетей, архитектура сети предложенная Оксфордским университетом в 2014 году \cite{simonyan2014vgg}. Основное различие между AlexNet и VGG заключается в размере фильтров, глубине сети и количестве параметров. AlexNet использует более крупные фильтры, например, 11×11 в первом сверточном слое, тогда как VGG применяет исключительно небольшие 3×3 фильтры, этот приём значительно увеличил глубину сети. Младшая модель VGG 11 имеет 9 сверточных и 2 полносвязаных слоя, и содержит в себе более 130 миллионов параметров, в то время как у AlexNet их всего 60. Это сделало VGGNet тяжеловесными и требовательными к вычислительным ресурсам.

Архитектура \textbf{Inception} (GoogLeNet) - её ключевой особенностью являются Inception-блоки, которые параллельно применяют сверточные операции с разными размерами фильтров (1×1, 3×3, 5×5) вместе с операциями max-pooling. Это позволяет сети извлекать признаки на разных уровнях детализации одновременно \cite{szegedy2014going}. Архитектура активно использует сверточные слои 1×1 для уменьшения размерности данных, что помогло снизить вычислительную сложность и количество параметров.

Каждая новая архитектура увеличивалась в размере, и поэтмоу появилась проблема, что градиенты на последних слоях были совсем маленькими, и это не давало нормально обновить веса модели. \textbf{ResNet} (Residual Networks) решила проблему увеличения глубины сетей \cite{he2015deep}. Авторы ResNet использовали "остаточные блоки" с соединениями быстрого доступа (skip connections), которые позволяют градиентам эффективно распространяться через многие слои. Входные данные передаются по дополнительному соединению в обход следующих слоев и добавляются к полученному результату, это соединение не добавляет дополнительные параметры в сеть, поэтому ее структура не усложняется.

Появилась необходимость использования нейронных сетей на мобильных устройствах, и в 2017 году исследователями из Google была представлена архитектура  \textbf{MobileNet} \cite{howard2017mobilenets}. Основным отличием  MobileNet является использование глубоких отделимых сверточных слоев (depthwise separable convolutions), которые разделяют стандартную свертку на две операции - глубинную свертку (depthwise convolution), которая выполняет фильтрацию, применяя один фильтр для каждого входного канала и поточечную свертку (pointwise convolution) с фильтром 1×1, которая выполняет комбинирование выходных каналов. Этот подход значительно снижает вычислительную сложность и количество параметров по сравнению с обычными сверточными слоями. MobileNetV1 содержит около 4,2 млн параметров и показывает 70,6\% top-1 на соревновании ImageNet при значительно меньших вычислительных затратах по сравнению с предыдущими сетями.

Авторы обнаружили, что для достижения оптимальной производительности необходимо сбалансированное масштабирование 3 измерений сети: глубины (количества слоев), ширины (количества каналов), разрешения (размера входного изображения). В отличие от предыдущих подходов, которые обычно масштабировали только одно из этих измерений, авторы предлагают составное масштабирование (compound scaling). Из существующего метода под названием «Neural Architecture Search» \cite{zoph2017neural} для автоматического создания новых сетей и своего собственного метода масштабирования авторы получают новый класс моделей под названием  \textbf{EfficientNet} \cite{tan2019efficientnet}. EfficientNet-B0 содержит около 5,3 млн параметров и достигает точности 77,1\% top-1 на ImageNet. Более крупные версии демонстрируют еще более высокую точность при контролируемом увеличении количества параметров: например, EfficientNet-B7 достигает 84,4\% с более чем 60 млн параметров, что является State-Of-The-Art разработкой до сих пор. 


Обращая внимание на архитектуры основанных на трансформерах, стоит упомянуть \textbf{Vision Transformer} (ViT), предложенный в 2020 году \cite{dosovitskiy2020image}, адаптирует архитектуру трансформера для изображений. ViT разбивает изображение на последовательность непересекающихся патчей, преобразует их в эмбеддинги и подает эту последовательность на вход стандартного трансформера. ViT демонстрирует впечатляющие результаты при обучении на больших наборах данных (ViT-L/16 получил 85.30\% top-1 точности после предобучения на датасете JFT-300M), но уступает CNN на меньших датасетах (ViT-B/16 показал слабые результаты при обучении только на ImageNet-1k без предварительного обучения на больших наборах данных, 79,9\% top-1 точности на ImageNet, что уступало современным CNNs) и требует значительных вычислительных ресурсов для обучения.

Также важно отметить семейство моделей \textbf{YOLO} (You Only Look Once), разработанных специально для детекции объектов в изображениях в режиме реального времени \cite{redmon2015you}. В отличие от обычных CNN, которые часто используются для классификации изображений или извлечения признаков, YOLO объединяет в себе процессы детекции объектов и их классификации, обрабатывая изображение целиком за один проход. Традиционно многие системы сначала выделяют регионы интереса, а затем отдельно классифицируют содержимое этих регионов. В случае YOLO всё это происходит за один шаг - сеть делит изображение на сетку и для каждой ячейки одновременно предсказывает координаты прямоугольника и вероятность того, что в нём находится объект определённого класса. Это объединение позволяет существенно ускорить задачу классификации. \textbf{YOLOv11} \cite{Khanam2024yolov11} является одной из последних моделей этой серии, конкурируя с классическими CNNs.

\textbf{Обоснование выбора архитектур для работы}

\hspace{1cm}
Для задач класса \textit{Few-Shot Learning} - необходимо выбрать такие архитектуры, которые обеспечат баланс между точности, скорости обучения и возможности обучаться на небольших данных. На основе проведенного анализа современных архитектур, особенно учитывая требование адаптации к малым объемам данных, для экспериментов выбраны две архитектуры:
\begin{enumerate}
\item EfficientNet

\begin{itemize}[leftmargin=-0.5cm]
    \item Эффективность использования параметров - отличные метрики при относительно небольшом количестве параметров
    \item Масштабируемость - семейство моделей от B0 до B7 позволяет выбрать оптимальный баланс между точностью и сложностью
    \item Меньшая требовательность к вычислительным ресурсам - можно эффективно использовать облачную инфраструктуру Yandex Cloud и быстрый локальный инференс
    \item Простая поддержка и создание пайплайна для обучения, так как EfficientNet уже есть в базовых классах PyTorch.
\end{itemize}
\item YOLO
\begin{itemize}[leftmargin=-0.5cm]
    \item Универсальность - может использоваться как для классификации, так и для обнаружения объектов. (Удобно для развертывания в дальнейших задач проекта)
    \item Высокая скорость работы - однопроходная архитектура обеспечивает быстрый инференс, но и обучение. 
    \item Точность --- последние версии (YOLOv11) показывают конкурентные результаты по сравнению с другими современными моделями.
    \item Масштабируемость --- доступны варианты разного размера (nano, small, medium, large, xlarge)
\end{itemize}
\end{enumerate}

\subsection{Агментации данных и реализация процесса обучения моделей}
\section*{Список использованной литературы}
\printbibliography[heading=none]
\end{document}